#!/bin/sh
# memU-server è‡ªå®šä¹‰å¯åŠ¨è„šæœ¬
# åœ¨å¯åŠ¨å‰ patch main.pyï¼Œæ³¨å…¥ embed_model é…ç½®

MAIN_PY="/app/app/main.py"
EMBED_MODEL="${DEFAULT_EMBED_MODEL:-nomic-embed-text}"
CHAT_MODEL="${DEFAULT_LLM_MODEL:-qwen2.5:0.5b}"
OLLAMA_URL="${OPENAI_BASE_URL:-http://host.docker.internal:11434/v1}"

# 1. ç­‰å¾… Ollama å¯ç”¨
echo "â³ ç­‰å¾… Ollama ($OLLAMA_URL) å¯ç”¨..."
for i in $(seq 1 30); do
    if python3 -c "
import urllib.request
try:
    r = urllib.request.urlopen('${OLLAMA_URL}/models', timeout=5)
    print('âœ… Ollama å¯ç”¨')
    exit(0)
except Exception as e:
    exit(1)
" 2>/dev/null; then
        break
    fi
    echo "   é‡è¯• $i/30..."
    sleep 2
done

# 2. é¢„çƒ­ embedding æ¨¡å‹
echo "ğŸ”¥ é¢„çƒ­ embedding æ¨¡å‹: $EMBED_MODEL"
python3 -c "
import urllib.request, json
url = '${OLLAMA_URL}/embeddings'
data = json.dumps({'model': '$EMBED_MODEL', 'input': ['warmup']}).encode()
req = urllib.request.Request(url, data=data, headers={'Content-Type': 'application/json'})
try:
    r = urllib.request.urlopen(req, timeout=30)
    print('âœ… é¢„çƒ­å®Œæˆ')
except Exception as e:
    print(f'âš ï¸  é¢„çƒ­å¤±è´¥: {e}')
" 2>&1

# 3. Patch main.py æ³¨å…¥ embed_model å’Œ chat_model
if ! grep -q 'embed_model' "$MAIN_PY"; then
    echo "ğŸ”§ patch memU main.py: embed_model=$EMBED_MODEL, chat_model=$CHAT_MODEL"
    # åœ¨ "model": ... è¡Œåæ’å…¥ embed_model
    sed -i "/\"model\": os.getenv/a\\            \"embed_model\": \"$EMBED_MODEL\"," "$MAIN_PY"
    # æ›¿æ¢é»˜è®¤ chat model ä¸º Ollama ä¸Šçš„æ¨¡å‹
    sed -i "s/gpt-4o-mini/$CHAT_MODEL/g" "$MAIN_PY"
    echo "   âœ… patch å®Œæˆ"
else
    echo "â„¹ï¸  embed_model å·²å­˜åœ¨ï¼Œè·³è¿‡ patch"
fi

# 4. æ˜¾ç¤º patched çš„ llm_profiles éƒ¨åˆ†
echo "ğŸ“‹ patched main.py llm_profiles:"
grep -A8 'llm_profiles' "$MAIN_PY" | head -10

# 5. ä½¿ç”¨ uvicorn ç›´æ¥å¯åŠ¨ï¼Œé¿å… gunicorn worker boot çš„ asyncio.run å…¼å®¹æ€§é—®é¢˜
echo "ğŸš€ å¯åŠ¨ uvicorn..."
exec python3 -m uvicorn app.main:app --host 0.0.0.0 --port 8000
