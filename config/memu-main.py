"""
è‡ªå®šä¹‰ memU-server å…¥å£æ–‡ä»¶
Hybrid æ–¹æ¡ˆï¼šZhipu GLM-4.5-Air åš ALL chat è°ƒç”¨ï¼ŒOllama åš embedding
"""

import json
import os
import traceback
import uuid
from pathlib import Path
from typing import Any, Dict

import httpx
from openai import AsyncOpenAI
from fastapi import FastAPI, HTTPException
from fastapi.responses import JSONResponse
from memu.app import MemoryService

app = FastAPI()

# ===== é…ç½® =====
# Ollama ç”¨äº embedding
ollama_base_url = os.getenv("OPENAI_BASE_URL", "http://172.17.0.1:11434/v1")
embed_model = os.getenv("DEFAULT_EMBED_MODEL", "nomic-embed-text")

# Zhipu ç”¨äºæ‰€æœ‰ LLM chat è°ƒç”¨
zhipu_api_key = os.getenv("ZHIPU_API_KEY", "")
zhipu_base_url = os.getenv("ZHIPU_BASE_URL", "https://open.bigmodel.cn/api/coding/paas/v4")
chat_model = os.getenv("DEFAULT_LLM_MODEL", "glm-4.5-air")

# Ollama ä¸Šå¯ç”¨çš„ fallback chat æ¨¡å‹ï¼ˆç”¨äºéå…³é”®è°ƒç”¨ï¼‰
ollama_chat_model = os.getenv("OLLAMA_CHAT_MODEL", "qwen2.5:1.5b")

print(f"ğŸ”§ memU hybrid é…ç½®:")
print(f"   Chat:  {zhipu_base_url} / {chat_model}")
print(f"   Embed: {ollama_base_url} / {embed_model}")
print(f"   Ollama chat fallback: {ollama_chat_model}")

# åˆå§‹åŒ– MemoryService
# å…³é”®ä¿®å¤ï¼šchat_model å¿…é¡»æ˜¯ Ollama ä¸Šå®é™…å­˜åœ¨çš„æ¨¡å‹
# å› ä¸º MemoryService å†…éƒ¨çš„æŸäº›æ–¹æ³•ç›´æ¥è°ƒç”¨ self.openai.clientï¼ˆæŒ‡å‘ Ollamaï¼‰
service = MemoryService(
    llm_config={
        "api_key": "ollama",
        "base_url": ollama_base_url,
        "chat_model": ollama_chat_model,  # ä½¿ç”¨ Ollama ä¸Šå®é™…å­˜åœ¨çš„æ¨¡å‹
        "embed_model": embed_model,
    }
)

# å¢åŠ  Ollama client è¶…æ—¶ï¼ˆCPU æ¨ç†ï¼‰
service.openai.client.timeout = httpx.Timeout(connect=30.0, read=120.0, write=120.0, pool=120.0)

# åˆ›å»º Zhipu chat client
zhipu_client = AsyncOpenAI(
    api_key=zhipu_api_key,
    base_url=zhipu_base_url,
    timeout=httpx.Timeout(connect=10.0, read=120.0, write=120.0, pool=120.0),
)

# Monkey-patch: è®©æ‰€æœ‰ summarize è°ƒç”¨èµ° Zhipuï¼ˆè´¨é‡æ›´é«˜ï¼‰
# å…¶ä»–ç›´æ¥çš„ chat è°ƒç”¨ä¼šèµ° Ollama fallback æ¨¡å‹
import types


async def _zhipu_summarize(self, text, *, max_tokens=None, system_prompt=None):
    """ä½¿ç”¨ Zhipu åš summarizeï¼ˆæ ¸å¿ƒè®°å¿†æå–æ–¹æ³•ï¼‰"""
    prompt = system_prompt or "Summarize the text in one short paragraph."
    messages = [
        {"role": "system", "content": prompt},
        {"role": "user", "content": text},
    ]
    response = await zhipu_client.chat.completions.create(
        model=chat_model,
        messages=messages,
        temperature=1,
        max_tokens=max_tokens,
    )
    return response.choices[0].message.content or ""


service.openai.summarize = types.MethodType(_zhipu_summarize, service.openai)

print(f"âœ… Hybrid é…ç½®å®Œæˆ: summarize â†’ Zhipu, chat fallback â†’ Ollama, embedding â†’ Ollama")

# å¯¹è¯æ–‡ä»¶å­˜å‚¨ç›®å½•
storage_dir = Path(os.getenv("MEMU_STORAGE_DIR", "./data"))
storage_dir.mkdir(parents=True, exist_ok=True)


@app.post("/memorize")
async def memorize(payload: Dict[str, Any]):
    try:
        file_path = storage_dir / f"conversation-{uuid.uuid4().hex}.json"
        with file_path.open("w", encoding="utf-8") as f:
            json.dump(payload, f, ensure_ascii=False)

        result = await service.memorize(resource_url=str(file_path), modality="conversation")
        return JSONResponse(content={"status": "success", "result": result})
    except Exception as exc:
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=str(exc))


@app.post("/retrieve")
async def retrieve(payload: Dict[str, Any]):
    if "query" not in payload:
        raise HTTPException(status_code=400, detail="Missing 'query' in request body")
    try:
        result = await service.retrieve([payload["query"]])
        return JSONResponse(content={"status": "success", "result": result})
    except Exception as exc:
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=str(exc))


@app.get("/")
async def root():
    return {"message": "Hello MemU user!"}
